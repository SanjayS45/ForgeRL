# Language-Conditioned RLHF for MetaWorld Robotics

A complete **Reinforcement Learning from Human Feedback (RLHF)** pipeline for training robotic manipulation policies using MetaWorld V3 environments. The system supports language-conditioned tasks, allowing natural language instructions to guide robot behavior.

## ğŸ¯ Features

- **MetaWorld V3 Integration**: Properly configured Sawyer arm environments with correct Task/Env handling
- **Reward Model Training**: Neural reward models with sentence transformer encodings for language conditioning
- **PPO Policy Training**: Clean PPO implementation with reward model integration
- **React Dashboard**: Modern web UI for monitoring training and managing experiments
- **FastAPI Backend**: REST API for programmatic control of the training pipeline
- **Flexible Dataset Support**: Load any dataset of trajectory preference pairs

## ğŸ“ Project Structure

```
language_rlhf_robotics/
â”œâ”€â”€ backend/              # FastAPI backend
â”‚   â””â”€â”€ api.py           # REST API endpoints
â”œâ”€â”€ checkpoints/         # Model checkpoints
â”œâ”€â”€ envs/                # Environment wrappers
â”‚   â”œâ”€â”€ metaworld_wrapper.py  # MetaWorld V3 wrapper
â”‚   â”œâ”€â”€ language.py      # Sentence transformer encoding
â”‚   â””â”€â”€ instructions.py  # Task instructions
â”œâ”€â”€ experiments/         # Datasets and experiment files
â”œâ”€â”€ frontend/            # React dashboard
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ App.jsx      # Main React application
â”œâ”€â”€ logs/                # Training logs
â”œâ”€â”€ policy/              # RL policy code
â”‚   â”œâ”€â”€ networks.py      # Actor-Critic networks
â”‚   â””â”€â”€ ppo.py          # PPO algorithm
â”œâ”€â”€ preferences/         # Preference labeling
â”‚   â””â”€â”€ synthetic.py     # Synthetic preference oracle
â”œâ”€â”€ reward_model/        # Reward model
â”‚   â”œâ”€â”€ model.py        # Neural reward models
â”‚   â””â”€â”€ trainer.py      # Training utilities
â”œâ”€â”€ scripts/             # Utility scripts
â”‚   â”œâ”€â”€ generate_trajectories.py  # Dataset generation
â”‚   â”œâ”€â”€ preprocess_dataset.py     # Data preprocessing
â”‚   â””â”€â”€ test_env.py               # Environment testing
â”œâ”€â”€ training/            # Training scripts
â”‚   â”œâ”€â”€ train_reward_model.py     # Reward model training
â”‚   â””â”€â”€ train_policy.py           # Policy training
â”œâ”€â”€ utils/               # Utilities
â”‚   â”œâ”€â”€ data_utils.py    # Dataset loading/saving
â”‚   â”œâ”€â”€ instruction_encoder.py   # Text encoding
â”‚   â””â”€â”€ logging_utils.py # Logging and checkpointing
â””â”€â”€ requirements.txt     # Python dependencies
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repo-url>
cd language_rlhf_robotics

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Test Environment Setup

Verify MetaWorld is correctly installed:

```bash
python scripts/test_env.py --task reach-v3 --test-wrapper
```

### 3. Generate Training Dataset

Generate 1000+ preference pairs:

```bash
python scripts/generate_trajectories.py \
    --task reach-v3 \
    --num-pairs 1000 \
    --horizon 50 \
    --output experiments/trajs.pkl
```

### 4. Train Reward Model

```bash
python training/train_reward_model.py \
    --data experiments/trajs.pkl \
    --epochs 50 \
    --batch-size 32 \
    --checkpoint-dir checkpoints/reward_model
```

### 5. Train Policy with Learned Reward

```bash
python training/train_policy.py \
    --task reach-v3 \
    --reward-model checkpoints/reward_model/best_model.pt \
    --instruction "reach the target" \
    --total-steps 500000
```

## ğŸ–¥ï¸ Web Dashboard

### Start Backend

```bash
cd language_rlhf_robotics
uvicorn backend.api:app --reload --port 8000
```

### Start Frontend

```bash
cd frontend
npm install
npm run dev
```

Access the dashboard at `http://localhost:5173`

### Deploy to Netlify

1. Build the frontend:
   ```bash
   cd frontend
   npm run build
   ```

2. Deploy the `dist` folder to Netlify

3. Update `netlify.toml` with your backend URL

## ğŸ“¦ Available Datasets

Pre-generated datasets for training:

| Dataset | Task | Pairs | Description |
|---------|------|-------|-------------|
| `reach_v3_large.pkl` | reach-v3 | 5,000 | Large reach task dataset |
| `push_v3_dataset.pkl` | push-v3 | 2,000 | Push task dataset |
| `pick_place_v3_dataset.pkl` | pick-place-v3 | 2,000 | Pick and place dataset |
| `door_open_v3_dataset.pkl` | door-open-v3 | 1,500 | Door opening dataset |
| `drawer_open_v3_dataset.pkl` | drawer-open-v3 | 1,500 | Drawer opening dataset |
| `multi_task_9k.pkl` | Multiple | 9,000 | Combined multi-task dataset |

### Generate Custom Dataset

```bash
python scripts/generate_large_dataset.py \
    --task reach-v3 \
    --num-pairs 5000 \
    --output experiments/my_dataset.pkl
```

### Combine Datasets

```bash
python scripts/combine_datasets.py \
    --inputs experiments/reach*.pkl experiments/push*.pkl \
    --output experiments/combined.pkl
```

## ğŸ“Š Dataset Format

Preference datasets use the format:
```python
[
    (instruction: str, trajectory_a: List[Tuple], trajectory_b: List[Tuple]),
    ...
]
```

Where each trajectory is:
```python
[(observation, action), (observation, action), ...]
```

Supported file formats: `.pkl`, `.pickle`, `.json`

## ğŸ”§ Configuration

### Reward Model Training

| Parameter | Default | Description |
|-----------|---------|-------------|
| `epochs` | 50 | Training epochs |
| `batch_size` | 32 | Batch size |
| `learning_rate` | 1e-4 | Learning rate |
| `hidden_dim` | 256 | Hidden layer dimension |
| `loss_type` | cross_entropy | Loss function |

### Policy Training (PPO)

| Parameter | Default | Description |
|-----------|---------|-------------|
| `total_steps` | 500000 | Total training steps |
| `steps_per_update` | 2048 | Steps between updates |
| `epochs_per_update` | 10 | PPO epochs per update |
| `gamma` | 0.99 | Discount factor |
| `clip_ratio` | 0.2 | PPO clip ratio |

## ğŸ”Œ API Reference

### Datasets

- `GET /api/datasets` - List available datasets
- `POST /api/datasets/upload` - Upload new dataset
- `DELETE /api/datasets/{name}` - Delete dataset

### Training

- `POST /api/reward-model/train` - Start reward model training
- `POST /api/policy/train` - Start policy training
- `GET /api/training/{job_id}` - Get training status
- `GET /api/training/{job_id}/history` - Get training history

### Models

- `GET /api/models` - List trained models
- `GET /api/models/{type}/{name}/download` - Download model

## ğŸ› Common Issues

### MetaWorld Task vs Env Error

**Problem**: `'Task' object has no attribute 'reset'`

**Solution**: Use `ml1.train_classes[task_name]` to get the environment class, not `ml1.train_tasks[0]`. The `train_tasks` list contains Task configuration objects, not environments.

```python
# âŒ Wrong
env = ml1.train_tasks[0]

# âœ… Correct
env_cls = ml1.train_classes['reach-v3']
env = env_cls()
task = ml1.train_tasks[0]
env.set_task(task)
```

### Import Errors

Run all scripts from the project root:

```bash
cd language_rlhf_robotics
python scripts/generate_trajectories.py
```

### CUDA Out of Memory

Reduce batch size or use CPU:

```bash
python training/train_reward_model.py --batch-size 16 --device cpu
```

## ğŸ“ Citation

If you use this codebase, please cite:

```bibtex
@software{language_rlhf_robotics,
  title = {Language-Conditioned RLHF for MetaWorld Robotics},
  year = {2024},
  url = {https://github.com/your-repo}
}
```

## ğŸ“„ License

MIT License - see LICENSE file for details.

